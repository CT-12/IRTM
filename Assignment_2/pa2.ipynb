{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import string\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_file(file_path: str) -> dict[str, list[str]]:\n",
    "    # 檢查該路徑是否為目錄\n",
    "    if not os.path.isdir(file_path):\n",
    "        print(\"The path is not a directory.\")\n",
    "        sys.exit(1)\n",
    "    # 取得所有目錄下檔案名稱\n",
    "    file_names: list[str] = os.listdir(file_path)\n",
    "    # 依照檔案名稱排序\n",
    "    file_names.sort(key=lambda x: int(x.split(\".\")[0]))\n",
    "    # 取得所有檔案內容\n",
    "    all_content: dict[str, list[str]] = {\"File Name\": [], \"Content\": []}\n",
    "    for file_name in file_names:\n",
    "        fd = open(os.path.join(file_path, file_name), \"r\")\n",
    "        content: list[str] = fd.readlines()\n",
    "        fd.close()    \n",
    "        all_content[\"File Name\"].append(file_name)\n",
    "        all_content[\"Content\"].append(\" \".join(content))\n",
    "\n",
    "    return all_content\n",
    "\n",
    "def read_file(file_path: str) -> list[str]:\n",
    "    fd = open(file_path, \"r\")\n",
    "    contents = fd.readlines()\n",
    "    fd.close()\n",
    "    \n",
    "    return [content.replace(\"\\n\", \"\") for content in contents]\n",
    "\n",
    "def write_file(file_path: str, data: list[float]):\n",
    "    fd = open(file_path, \"w\")\n",
    "    for d in data:\n",
    "        fd.write(str(d)+\"\\n\")\n",
    "    fd.close()\n",
    "\n",
    "def text_preprocessing(text: str, stopwords: list[str]) -> str:\n",
    "    # 移除標點符號\n",
    "    punctuations = string.punctuation + '-'\n",
    "    text = \"\".join([char for char in text if char not in punctuations])\n",
    "    # 去除數字\n",
    "    text: str = re.sub(r\"\\d+\", \"\", text)\n",
    "    # 轉換為小寫\n",
    "    text: str = text.lower()\n",
    "    # 去除 stopwords\n",
    "    text: list[str] = text.split()\n",
    "    text: list[str] = [t for t in text if t not in stopwords]\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    text: list[str] = [stemmer.stem(t) for t in text]\n",
    "\n",
    "\n",
    "    text: str = \" \".join(text)\n",
    "    return text    \n",
    "\n",
    "def compute_cosine_similarity(doc_x: str, doc_y: str, dictionary: pd.DataFrame) -> float:\n",
    "    doc_x_df = pd.read_csv(\"./output/\"+doc_x, sep='\\t')\n",
    "    doc_y_df = pd.read_csv(\"./output/\"+doc_y, sep='\\t')\n",
    "\n",
    "    # 去除 t_index 重複的列，以防合併後數量不一致\n",
    "    doc_x_df.drop_duplicates(subset='t_index', inplace=True)\n",
    "    doc_y_df.drop_duplicates(subset='t_index', inplace=True)\n",
    "    # 將 dataframe 合併 \n",
    "    merge_df = pd.merge(dictionary, doc_x_df, on='t_index', how='left')\n",
    "    merge_df.fillna(0, inplace=True)\n",
    "    doc_x_vector = merge_df['tf_idf'].values\n",
    "\n",
    "    merge_df = pd.merge(dictionary, doc_y_df, on='t_index', how='left')\n",
    "    merge_df.fillna(0, inplace=True)\n",
    "    doc_y_vector = merge_df['tf_idf'].values\n",
    "\n",
    "    return np.dot(doc_x_vector, doc_y_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"./data\"\n",
    "all_content: dict[str, list[str]] = read_all_file(input_file_path)\n",
    "all_content_df: pd.DataFrame = pd.DataFrame(all_content)\n",
    "stopwords = read_file(\"./stopwords.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content_df[\"Content\"] = all_content_df[\"Content\"].apply(text_preprocessing, args=(stopwords,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Document Frequency(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for text in all_content_df[\"Content\"].values:\n",
    "    tokens = list(set(text.split(\" \")))\n",
    "    for term in tokens:\n",
    "        if term in dictionary:\n",
    "            dictionary[term] += 1\n",
    "        else:\n",
    "            dictionary[term] = 1\n",
    "\n",
    "# 依照字典 key 排序\n",
    "dictionary = dict(sorted(dictionary.items()))\n",
    "# 轉換成 DataFrame\n",
    "dictionary = pd.DataFrame.from_dict(dictionary, orient=\"index\", columns=[\"df\"])\n",
    "dictionary.reset_index(inplace=True)\n",
    "dictionary = dictionary.rename(columns={\"index\": \"term\"})\n",
    "dictionary.reset_index(inplace=True)\n",
    "dictionary = dictionary.rename(columns={\"index\": \"t_index\"})\n",
    "# Output\n",
    "dictionary.to_csv('./output/dictionary.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Document Frequency(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = len(all_content_df)\n",
    "dictionary[\"idf\"] = np.log10(number_of_documents / dictionary[\"df\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [08:07<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for index in trange(number_of_documents):\n",
    "    tf_idf = {}\n",
    "    row = all_content_df.iloc[index]\n",
    "    tokens = row[\"Content\"].split(\" \")\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "         # 計算 tf\n",
    "        tf = tokens.count(token) / len(tokens)\n",
    "        # 計算 tf-idf\n",
    "        tf_idf[token] = tf * dictionary[dictionary[\"term\"]==token][\"idf\"].values[0]\n",
    "    \n",
    "    # 根據算出來的 tf-idf 組成文件的向量\n",
    "    doc_vec = {\"t_index\": [], \"tf_idf\": []}\n",
    "    for token in tokens:\n",
    "        doc_vec[\"t_index\"].append(dictionary[dictionary[\"term\"] == token][\"t_index\"].values[0])\n",
    "        doc_vec[\"tf_idf\"].append(tf_idf[token])\n",
    "    doc_vec_df = pd.DataFrame(doc_vec)\n",
    "    # 轉成 unit vector\n",
    "    doc_vec_df['tf_idf'] = doc_vec_df['tf_idf'] / np.linalg.norm(doc_vec_df['tf_idf'].values)\n",
    "    # Output\n",
    "    doc_vec_df.to_csv(f'./output/{row[\"File Name\"]}', sep='\\t', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.07202953986765596\n"
     ]
    }
   ],
   "source": [
    "doc_x = \"1.txt\"\n",
    "doc_y = \"2.txt\"\n",
    "\n",
    "cos_similarity = compute_cosine_similarity(doc_x, doc_y, dictionary)\n",
    "print(f\"Cosine Similarity: {cos_similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

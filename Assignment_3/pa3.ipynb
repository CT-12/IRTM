{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>train or test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the white house is also keeping a close watch ...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>turning to news overseas, a tense political sh...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Pressing a strategy of legal challenges and po...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In Yugoslavia, the Democratic opposition will ...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>The Yugoslavia opposition is urging its suppor...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>1091</td>\n",
       "      <td>In western India Wednesday rescue workers pull...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>1092</td>\n",
       "      <td>While the International relief effort and some...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>1093</td>\n",
       "      <td>As the death toll from last week's devastating...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>1094</td>\n",
       "      <td>A massive international relief effort continue...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>1095</td>\n",
       "      <td>The split verdict Wednesday in the trial of tw...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                               text label  \\\n",
       "1         1  the white house is also keeping a close watch ...     2   \n",
       "2         2  turning to news overseas, a tense political sh...     2   \n",
       "3         3  Pressing a strategy of legal challenges and po...     2   \n",
       "4         4  In Yugoslavia, the Democratic opposition will ...     2   \n",
       "5         5  The Yugoslavia opposition is urging its suppor...     2   \n",
       "...     ...                                                ...   ...   \n",
       "1091   1091  In western India Wednesday rescue workers pull...     0   \n",
       "1092   1092  While the International relief effort and some...     0   \n",
       "1093   1093  As the death toll from last week's devastating...     0   \n",
       "1094   1094  A massive international relief effort continue...     0   \n",
       "1095   1095  The split verdict Wednesday in the trial of tw...     0   \n",
       "\n",
       "     train or test  \n",
       "1            train  \n",
       "2            train  \n",
       "3            train  \n",
       "4            train  \n",
       "5            train  \n",
       "...            ...  \n",
       "1091          test  \n",
       "1092          test  \n",
       "1093          test  \n",
       "1094          test  \n",
       "1095          test  \n",
       "\n",
       "[1095 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_doc = 1095\n",
    "\n",
    "with open(\"./data/training.txt\", 'r') as f:\n",
    "    trainset_data = f.readlines()\n",
    "    trainset_data: list[list[int]] = [list(map(int, train.strip(\" \\n\").split()) )for train in trainset_data]\n",
    "    trainset_data: dict[int, list[int]] = {train[0]: train[1:] for train in trainset_data}\n",
    "\n",
    "dataset: pd.DataFrame = pd.DataFrame(index=range(1, num_of_doc+1), columns=['doc_id', 'text', 'label', 'train or test'])\n",
    "\n",
    "# 把所有 doc 以及他們的 text 放入 dataframe\n",
    "for i in range(1, num_of_doc+1):\n",
    "    with open(f\"./data/{i}.txt\", 'r') as f:\n",
    "        text = f.read()\n",
    "        dataset.loc[i, \"doc_id\"] = i\n",
    "        dataset.loc[i, \"text\"] = text\n",
    "\n",
    "# 把 doc 放入對應的 label\n",
    "for label in trainset_data:\n",
    "    for doc_id in trainset_data[label]:\n",
    "        dataset.loc[doc_id, \"label\"] = label\n",
    "        dataset.loc[doc_id, \"train or test\"] = 'train'\n",
    "\n",
    "# 把沒有 label 的 doc 標記為 test\n",
    "dataset.loc[dataset[dataset[\"label\"].isnull()].index, \"train or test\"] = \"test\"\n",
    "# 把沒有 test doc 的 label 標記為 0\n",
    "dataset.loc[dataset[dataset[\"label\"].isnull()].index, \"label\"] = 0\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split trainset, validset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 195\n",
      "testset size: 900\n"
     ]
    }
   ],
   "source": [
    "trainset: pd.DataFrame = dataset[dataset[\"train or test\"] == \"train\"]\n",
    "# subtrainset: pd.DataFrame = trainset.sample(frac=0.8, random_state=12)\n",
    "# validset: pd.DataFrame = trainset.drop(subtrainset.index)\n",
    "testset: pd.DataFrame = dataset[dataset[\"train or test\"] == \"test\"]\n",
    "\n",
    "# print(f\"subtrainset size: {subtrainset.shape[0]}\")\n",
    "# print(f\"validset size: {validset.shape[0]}\")\n",
    "print(f\"trainset size: {trainset.shape[0]}\")\n",
    "print(f\"testset size: {testset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stopwords.txt\", 'r') as f:\n",
    "    stopwords = f.readlines()\n",
    "    stopwords: list[str] = [stopword.replace(\"\\n\", \"\") for stopword in stopwords]\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    punctuation = string.punctuation + '-'\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "def remove_number(text: str) -> str:\n",
    "    return re.sub(r\"\\d+\", '', text)\n",
    "\n",
    "def lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(text: list[str]) -> list[str]:\n",
    "    return [token for token in text if token not in stopwords]\n",
    "\n",
    "def stemming(text: list[str]) -> list[str]:\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in text]\n",
    "\n",
    "def text_preprocessing_pipeline(text: str) -> list[str]:\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_number(text)\n",
    "    text = lowercase(text)\n",
    "    text = tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    return text\n",
    "\n",
    "# subtrainset.loc[:, \"text\"] = subtrainset.loc[:, \"text\"].apply(text_preprocessing_pipeline)\n",
    "# validset.loc[:, \"text\"] = validset.loc[:, \"text\"].apply(text_preprocessing_pipeline)\n",
    "trainset.loc[:, \"text\"] = trainset.loc[:, \"text\"].apply(text_preprocessing_pipeline)\n",
    "testset.loc[:, \"text\"] = testset.loc[:, \"text\"].apply(text_preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a vocabulary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(trainset: pd.DataFrame) -> list[str]:\n",
    "    dictionary = []\n",
    "    for i in trainset.index:\n",
    "        dictionary.extend(trainset.loc[i, \"text\"])\n",
    "    dictionary = sorted(list(set(dictionary)))\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute DF and IDF\n",
    "- Document Frequency(DF) \n",
    "- Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_to_df_and_idf: pd.DataFrame = pd.DataFrame(dictionary, columns=[\"term\"])\n",
    "\n",
    "term_to_df_and_idf.loc[:, \"df\"] = term_to_df_and_idf.loc[:, \"term\"].apply(\n",
    "    lambda term: subtrainset.loc[:, \"text\"].apply(lambda text: term in text).sum()\n",
    ")\n",
    "\n",
    "term_to_df_and_idf.loc[:, \"idf\"] = np.log(subtrainset.shape[0] / term_to_df_and_idf.loc[:, \"df\"])\n",
    "\n",
    "term_to_df_and_idf.set_index(\"term\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrainset.loc[:, \"tf-idf vector\"] = subtrainset.loc[:, \"text\"].apply(\n",
    "    lambda text: np.array([text.count(term) * term_to_df_and_idf.loc[term, \"idf\"] for term in dictionary])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Use Log Likelihood Ratio to do feature selection. Select top 500 terms for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5157/5157 [01:10<00:00, 72.93it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_contingency_table(trainset: pd.DataFrame, term: str) -> pd.DataFrame:\n",
    "    contingency_table = pd.DataFrame(index=trainset[\"label\"].unique(), columns=[\"present\", \"absent\"])\n",
    "\n",
    "    for c in contingency_table.index:\n",
    "        # 有多少在 class c 的文件包含 term\n",
    "        present = trainset[trainset[\"label\"] == c].loc[:, \"text\"].apply(\n",
    "            lambda text: term in text\n",
    "        ).sum()\n",
    "        # 有多少在 class c 的文件不包含 term\n",
    "        absent = trainset[trainset[\"label\"] == c].shape[0] - present\n",
    "        contingency_table.loc[c, :] = [present, absent]\n",
    "\n",
    "    return contingency_table\n",
    "\n",
    "def log_likelihoold_ratio(contingency_table: pd.DataFrame):\n",
    "    N = contingency_table.loc[:, \"present\"].sum() + contingency_table.loc[:, \"absent\"].sum()\n",
    "    pt = contingency_table.loc[:, \"present\"].sum() / N\n",
    "    p = dict()\n",
    "\n",
    "    for c in contingency_table.index:\n",
    "        p[c] = contingency_table.loc[c, \"present\"] / (contingency_table.loc[c, \"present\"] + contingency_table.loc[c, \"absent\"])\n",
    "\n",
    "    H1, H2 = 1, 1\n",
    "    for c in contingency_table.index:\n",
    "        H1 *= math.comb(contingency_table.loc[c, \"present\"]+contingency_table.loc[c, \"absent\"], contingency_table.loc[c, \"present\"]) * (pt ** contingency_table.loc[c, \"present\"]) * ((1 - pt) ** contingency_table.loc[c, \"absent\"])\n",
    "        H2 *= math.comb(contingency_table.loc[c, \"present\"]+contingency_table.loc[c, \"absent\"], contingency_table.loc[c, \"present\"]) * (p[c] ** contingency_table.loc[c, \"present\"]) * ((1 - p[c]) ** contingency_table.loc[c, \"absent\"])\n",
    "\n",
    "    LLR = -2 * np.log(H1 / H2)\n",
    "\n",
    "    return LLR\n",
    "\n",
    "def feature_selection(trainset: pd.DataFrame, top_k: int = 500):\n",
    "    dictionary: list[str] = make_dictionary(trainset)\n",
    "    selected_features = []\n",
    "\n",
    "    for term in tqdm(dictionary):\n",
    "        contingency_table = get_contingency_table(trainset, term)\n",
    "        LLR = log_likelihoold_ratio(contingency_table)\n",
    "        selected_features.append((term, LLR))\n",
    "\n",
    "    selected_features = sorted(selected_features, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [feature[0] for feature in selected_features[:top_k]]\n",
    "\n",
    "selected_terms = feature_selection(trainset=trainset)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def concat_all_text_in_class_c(trainset: pd.DataFrame, c: int) -> list[str]:\n",
    "    text = []\n",
    "    for i in trainset[trainset[\"label\"] == c].index:\n",
    "        text.extend(trainset.loc[i, \"text\"])\n",
    "        \n",
    "    return text\n",
    "\n",
    "def count_term_in_class(term: str, text_c: list[str]) -> int:\n",
    "    return text_c.count(term)\n",
    "\n",
    "def train_multiNomial_naive_bayes(trainset: pd.DataFrame, dictionary: list[str]) -> tuple[list[int], dict[int, float], pd.DataFrame]:\n",
    "    # 分類的類別\n",
    "    classes = trainset[\"label\"].unique().tolist()\n",
    "    M = len(dictionary)\n",
    "    N = num_of_doc\n",
    "\n",
    "    prior_prob = {}\n",
    "    # conditional probability: P(t|c), size is M *|C|\n",
    "    cond_prob = pd.DataFrame(np.zeros((M, len(classes))), index=dictionary, columns=classes)\n",
    "    for c in tqdm(classes):\n",
    "        # N_c: number of documents in class c\n",
    "        N_c = trainset[\"label\"].value_counts()[c]\n",
    "        prior_prob[c] = N_c / N\n",
    "        text_c: list[str] = concat_all_text_in_class_c(trainset, c)\n",
    "\n",
    "        for term in dictionary:\n",
    "            # T_ct: number of term t appears in class c\n",
    "            T_ct = count_term_in_class(term, text_c)\n",
    "            cond_prob.loc[term, c] = (T_ct + 1) / (len(text_c) + M)\n",
    "    \n",
    "    return classes, prior_prob, cond_prob\n",
    "\n",
    "classes, prior_prob, cond_prob = train_multiNomial_naive_bayes(trainset=trainset, dictionary=selected_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_doc: list[str], dictionary: list[str], classes: list[int], prior_prob: dict[int, float], cond_prob: pd.DataFrame) -> list[int]:\n",
    "    scores = {}\n",
    "\n",
    "    # W: 只留下有在 dictionary 裡面的 term, 其餘忽略掉\n",
    "    W = [term for term in test_doc if term in dictionary]\n",
    "\n",
    "    for c in classes:\n",
    "        scores[c] = np.log(prior_prob[c])\n",
    "\n",
    "        for term in W:\n",
    "            scores[c] += np.log(cond_prob.loc[term, c])\n",
    "    \n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "testset.loc[:, \"label\"] = testset.loc[:, \"text\"].apply(lambda text: inference(text, selected_terms, classes, prior_prob, cond_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\"Id\": testset[\"doc_id\"].values, \"Value\": testset[\"label\"].values}\n",
    "result = pd.DataFrame(result)\n",
    "result.to_csv(\"result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import string\n",
    "import math\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [00:00<00:00, 3260.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the white house is also keeping a close watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>turning to news overseas, a tense political sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Pressing a strategy of legal challenges and po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>In Yugoslavia, the Democratic opposition will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>The Yugoslavia opposition is urging its suppor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                           doc_text\n",
       "1       1  the white house is also keeping a close watch ...\n",
       "2       2  turning to news overseas, a tense political sh...\n",
       "3       3  Pressing a strategy of legal challenges and po...\n",
       "4       4  In Yugoslavia, the Democratic opposition will ...\n",
       "5       5  The Yugoslavia opposition is urging its suppor..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = {\n",
    "    'doc_id': [],\n",
    "    'doc_text': []\n",
    "}\n",
    "num_of_docs = 1095\n",
    "for i in tqdm(range(1, num_of_docs+1)):\n",
    "    with open(f'./data/{i}.txt', mode='r') as f:\n",
    "        doc_text: str = f.read()\n",
    "    \n",
    "    dataset['doc_id'].append(i)\n",
    "    dataset['doc_text'].append(doc_text)\n",
    "\n",
    "dataset: pd.DataFrame = pd.DataFrame(dataset, index=dataset['doc_id'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./stopwords.txt', mode='r') as f:\n",
    "    stopwords = f.read().split()\n",
    "\n",
    "def remove_newline_symbol(text: str) -> str:\n",
    "    return text.replace('\\n', '')\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    punctuation = string.punctuation + '-'\n",
    "\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "def remove_digital(text: str) -> str:\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "def lowercase(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def stemming(text: str) -> str:\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return text.split()\n",
    "\n",
    "def text_preprocessing_pipeline(text: str) -> list[str]:\n",
    "    text = remove_newline_symbol(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digital(text)\n",
    "    text = lowercase(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [00:10<00:00, 107.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>white hous keep close watch yugoslavia opposit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>news oversea tens polit showdown yugoslavia su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>press strategi legal challeng popular pressur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yugoslavia democrat opposit nationwid campaign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>yugoslavia opposit urg support civil disobedi ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                           doc_text\n",
       "1       1  white hous keep close watch yugoslavia opposit...\n",
       "2       2  news oversea tens polit showdown yugoslavia su...\n",
       "3       3  press strategi legal challeng popular pressur ...\n",
       "4       4  yugoslavia democrat opposit nationwid campaign...\n",
       "5       5  yugoslavia opposit urg support civil disobedi ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_datasets = dataset.copy()\n",
    "preprocessed_datasets['doc_text'] = preprocessed_datasets['doc_text'].progress_apply(text_preprocessing_pipeline)\n",
    "preprocessed_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [00:00<00:00, 31483.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lag', 'multifacet', 'buraika', 'jingqian', 'wouldv', 'earn', 'slice', 'spring', 'coverup', 'complianc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_dictionary(dataset: pd.DataFrame) -> list[str]:\n",
    "    dictionary = []\n",
    "    for doc_text in tqdm(dataset['doc_text']):\n",
    "        dictionary.extend(doc_text.split())\n",
    "    \n",
    "    return list(set(dictionary))\n",
    "\n",
    "dictionary = get_dictionary(preprocessed_datasets)\n",
    "print(dictionary[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get document frequency (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13189/13189 [00:29<00:00, 440.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lag</th>\n",
       "      <td>101</td>\n",
       "      <td>2.383389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multifacet</th>\n",
       "      <td>2</td>\n",
       "      <td>6.305362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buraika</th>\n",
       "      <td>1</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jingqian</th>\n",
       "      <td>1</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wouldv</th>\n",
       "      <td>1</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             df       idf\n",
       "term                     \n",
       "lag         101  2.383389\n",
       "multifacet    2  6.305362\n",
       "buraika       1  6.998510\n",
       "jingqian      1  6.998510\n",
       "wouldv        1  6.998510"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_df_and_idf(dataset: pd.DataFrame, dictionary: list[str]) -> pd.DataFrame:\n",
    "    term2df_and_idf: pd.DataFrame = pd.DataFrame(dictionary, columns=['term'])\n",
    "\n",
    "    term2df_and_idf['df'] = term2df_and_idf['term'].progress_apply(\n",
    "        lambda term: dataset['doc_text'].str.contains(term).sum()\n",
    "    )\n",
    "\n",
    "    term2df_and_idf['idf'] = np.log(len(dataset) / term2df_and_idf['df'])\n",
    "\n",
    "    term2df_and_idf.set_index('term', inplace=True)\n",
    "\n",
    "    return term2df_and_idf\n",
    "\n",
    "term2df_and_idf = compute_df_and_idf(preprocessed_datasets, dictionary)\n",
    "term2df_and_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get normalized tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [00:03<00:00, 334.05it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_tf_idf_vector(dataset: pd.DataFrame, term2df_and_idf: pd.DataFrame) -> dict[int, np.ndarray]:\n",
    "    doc2tf_idf_vector = dict()\n",
    "\n",
    "    for i in tqdm(range(1, len(dataset)+1)):\n",
    "        doc_text = dataset.loc[i, 'doc_text']\n",
    "\n",
    "        tf = pd.Series(doc_text.split()).value_counts().to_dict()\n",
    "        tf_vector: pd.Series = term2df_and_idf.index.map(tf).fillna(0)\n",
    "        tf_idf_vector = (tf_vector * term2df_and_idf['idf']) / np.linalg.norm(tf_vector * term2df_and_idf['idf'])\n",
    "       \n",
    "        doc2tf_idf_vector[i] = tf_idf_vector.values\n",
    "\n",
    "    return doc2tf_idf_vector\n",
    "\n",
    "doc2tf_idf_vector: dict[int, np.ndarray] = get_tf_idf_vector(preprocessed_datasets, term2df_and_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(doc1: np.ndarray, doc2: np.ndarray) -> float:\n",
    "    return np.dot(doc1, doc2) # doc 已經 normalize 過，所以不用再除以兩個 doc 向量的長度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing cluster similarity (complete link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_link(cluster_j: int, cluster_i: int, cluster_m: int, C: np.ndarray) -> float:\n",
    "    # 比較 i 與 j 的相似度，和新加入的 m 與 j 的相似度，取小的當作新的相似度\n",
    "    if C[cluster_i, cluster_j] < C[cluster_m, cluster_j]:\n",
    "        return C[cluster_i, cluster_j]\n",
    "    else:\n",
    "        return C[cluster_m, cluster_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heap:\n",
    "    def __init__(self):\n",
    "        self.data: list[tuple[float, int, int]] = [(-1, -1, -1)] # 0 號位置不放東西，方便之後的 heap 運算\n",
    "        self.n: int = 0 # 目前 heap 的大小\n",
    "    \n",
    "    def insert_data(self, x: tuple[float, int, int]): # tuple: [相似度, cluster_i, cluster_j]\n",
    "        self.n += 1\n",
    "        self.data.append(tuple())\n",
    "        idx = self.n\n",
    "\n",
    "        while idx > 1 and x[0] > self.data[math.floor(idx/2)][0]:\n",
    "            self.data[idx] = self.data[math.floor(idx/2)]\n",
    "            idx = math.floor(idx/2)\n",
    "        \n",
    "        self.data[idx] = x\n",
    "\n",
    "    def pop_data(self) -> tuple[float, int, int]:\n",
    "        x = self.data[1]\n",
    "        self.data[1] = self.data[self.n]\n",
    "        self.n -= 1\n",
    "\n",
    "        idx = 1\n",
    "        while idx <= (self.n/2):\n",
    "            if self.data[idx*2][0] > self.data[idx*2+1][0]:\n",
    "                child_idx = idx*2\n",
    "            else:\n",
    "                child_idx = idx*2+1\n",
    "\n",
    "            if self.data[idx][0] > self.data[child_idx][0]:\n",
    "                break\n",
    "\n",
    "            self.data[idx], self.data[child_idx] = self.data[child_idx], self.data[idx]\n",
    "            idx = child_idx\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(matrix: np.ndarray, I: np.ndarray) -> tuple[int, int]:\n",
    "    max_value = -np.inf\n",
    "    cluster_i = -1\n",
    "    cluster_m = -1\n",
    "    for i in range(len(matrix)):\n",
    "        if I[i] == 0: # 如果 cluster i 已經被 merge 到其他 cluster，就不用再看了\n",
    "            continue\n",
    "        for j in range(i+1, len(matrix)):\n",
    "            if I[j] == 0: # 如果 cluster j 已經被 merge 到其他 cluster，就不用再看了\n",
    "                continue\n",
    "\n",
    "            if matrix[i, j] > max_value:\n",
    "                max_value = matrix[i, j]\n",
    "                cluster_i = i\n",
    "                cluster_m = j\n",
    "    \n",
    "    return cluster_i, cluster_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficent_HAC(doc2tf_idf_vector: dict[int, np.ndarray]):\n",
    "    # C[i][j]: the similiarity between cluster i and cluster j\n",
    "    C = np.ones((len(doc2tf_idf_vector), len(doc2tf_idf_vector)))  \n",
    "    # I: indicate which clusters are still available to be merged\n",
    "    I = np.ones(len(doc2tf_idf_vector))\n",
    "    # A: log the merge history\n",
    "    A = []\n",
    "\n",
    "    heap = Heap()\n",
    "\n",
    "    # Initialize\n",
    "    for n in tqdm(range(len(doc2tf_idf_vector))):\n",
    "        for i in range(n+1, len(doc2tf_idf_vector)):\n",
    "            similarity = cosine_similarity(doc2tf_idf_vector[n+1], doc2tf_idf_vector[i+1])\n",
    "            # 這是一個對稱矩陣\n",
    "            C[n ,i] = similarity\n",
    "            C[i, n] = similarity\n",
    "\n",
    "            # Insert into heap\n",
    "            if n != i:\n",
    "                heap.insert_data((similarity, n, i))\n",
    "        \n",
    "        I[n] = 1\n",
    "    \n",
    "    # Merge: 進行 N-1 次 merge\n",
    "    for _ in tqdm(range(len(doc2tf_idf_vector)-1)): # 這邊 -1，因為 complete binary tree 有 N 個 external nodes (docs) 代表有 N-1 個 internal nodes (merge)\n",
    "        _, cluster_i, cluster_m = heap.pop_data()\n",
    "        while I[cluster_i] == 0 or I[cluster_m] == 0: # 如果 cluster_i 或 cluster_m 已經被 merge 過了，就再 pop 一次\n",
    "            _, cluster_i, cluster_m = heap.pop_data()\n",
    "        A.append((cluster_i+1, cluster_m+1)) # cluster_i 和 cluster_m 是從 0 開始算的，所以要 +1\n",
    "\n",
    "        # Update C: 因為 cluster_i merged 了 cluster_j，所以要重新計算新的 cluster 和其他 cluster 的相似度\n",
    "        for j in range(len(doc2tf_idf_vector)):\n",
    "            if j != cluster_i and j != cluster_m and I[j] == 1: # 不需要計算自己群跟自己群成員的相似度，也不需要計算已經被 merge 的群 (I[j] == 0)\n",
    "                new_similarity = complete_link(j, cluster_i, cluster_m, C)\n",
    "                C[cluster_i, j] = new_similarity\n",
    "                C[j, cluster_i] = new_similarity\n",
    "\n",
    "                heap.insert_data((new_similarity, cluster_i, j))\n",
    "\n",
    "        I[cluster_m] = 0 # cluster_m 被併掉了\n",
    "\n",
    "    return A\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1095/1095 [00:07<00:00, 141.55it/s]\n",
      "100%|██████████| 1094/1094 [00:14<00:00, 74.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(48, 49), (527, 529), (943, 944), (595, 596), (621, 622), (8, 9), (101, 106), (564, 565), (564, 595), (211, 212)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "A = efficent_HAC(doc2tf_idf_vector)\n",
    "print(A[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [8, 13, 20] # number of clusters\n",
    "def get_cluster(A: list[tuple[int, int]]):\n",
    "    for k in tqdm(K):\n",
    "        clusters = dict()\n",
    "        for i in range(num_of_docs-k): # num_of_docs-k 次 merge 後，就會有 k 個 clusters\n",
    "            cluster_i, cluster_m = A[i]\n",
    "            clusters[cluster_i] = clusters.get(cluster_i, []) + clusters.get(cluster_m, []) + [cluster_m]\n",
    "            if cluster_m in clusters:\n",
    "                del clusters[cluster_m]\n",
    "\n",
    "        # Write to file\n",
    "        with open(f'./{k}.txt', mode='w') as f:\n",
    "            for cluster in clusters:\n",
    "                c = sorted([cluster] + clusters[cluster])\n",
    "                for doc in c:\n",
    "                    f.write(f'{doc}\\n')\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 89.58it/s]\n"
     ]
    }
   ],
   "source": [
    "get_cluster(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
